{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandasql import sqldf\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from finance_byu.fama_macbeth import fama_macbeth, fm_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRSP = pd.read_sas('crsp.sas7bdat', encoding='latin-1') #fp3v1\n",
    "COMP = pd.read_sas('comp.sas7bdat', encoding='latin-1') #fp3v2\n",
    "FF4 = pd.read_sas('ff4data.sas7bdat', encoding='latin-1')\n",
    "COMP_ANNUAL = pd.read_sas('comp_annual.sas7bdat', encoding='latin-1') # fp3v3\n",
    "INDUSTRY =  pd.read_csv('industries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zero(df, var):\n",
    "    temp = df[var].isna().sum()\n",
    "    df[var].fillna(0, inplace=True)\n",
    "    print(\"NAN values for \" + str(var) + \": \" + str(temp))\n",
    "    temp = df[var].isna().sum()\n",
    "    print(\"NAN values for \" + str(var) + \" filled: \" + str(temp) + \" remaining\")\n",
    "\n",
    "def calculate_credit_metrics(compustat_data):\n",
    "    credit_metrics_df = pd.DataFrame()\n",
    "\n",
    "    # Rename \n",
    "    credit_metrics_df[\"Total_Assets\"] = compustat_data[\"AT\"]\n",
    "    credit_metrics_df[\"Current_Liabilities\"] = compustat_data[\"LCO\"]\n",
    "    credit_metrics_df[\"Long_Term_Debt\"] = compustat_data[\"DLTT\"]\n",
    "    credit_metrics_df[\"Total_Liabilities\"] = compustat_data[\"LT\"]\n",
    "    credit_metrics_df[\"Preferred_Stock\"] = compustat_data[\"PSTK\"]\n",
    "    credit_metrics_df[\"Interest_Expense\"] = compustat_data[\"XINT\"]\n",
    "    credit_metrics_df[\"Cash_Short_Term_Investments\"] = compustat_data[\"CHE\"]\n",
    "    credit_metrics_df[\"Net_Income\"] = compustat_data[\"NI\"]\n",
    "    credit_metrics_df[\"Inventory\"] = compustat_data[\"INVT\"]\n",
    "    credit_metrics_df[\"Total_Equity\"] = compustat_data[\"CEQ\"]\n",
    "\n",
    "    # Calculate financial ratios\n",
    "    credit_metrics_df[\"Debt_Equity_Ratio\"] = credit_metrics_df[\"Total_Liabilities\"] / credit_metrics_df[\"Total_Equity\"]\n",
    "    credit_metrics_df[\"Debt_Ratio\"] = credit_metrics_df[\"Total_Liabilities\"] / credit_metrics_df[\"Total_Assets\"]\n",
    "    credit_metrics_df[\"Current_Ratio\"] = credit_metrics_df[\"Current_Liabilities\"] / credit_metrics_df[\"Total_Assets\"]\n",
    "    credit_metrics_df[\"Quick_Ratio\"] = (credit_metrics_df[\"Current_Liabilities\"] - credit_metrics_df[\"Inventory\"]) / credit_metrics_df[\"Total_Assets\"]\n",
    "    credit_metrics_df[\"Interest_Coverage_Ratio\"] = credit_metrics_df[\"Net_Income\"] / credit_metrics_df[\"Interest_Expense\"]\n",
    "    credit_metrics_df[\"Return_On_Assets\"] = credit_metrics_df[\"Net_Income\"] / credit_metrics_df[\"Total_Assets\"]\n",
    "    credit_metrics_df[\"Return_On_Equity\"] = credit_metrics_df[\"Net_Income\"] / credit_metrics_df[\"Total_Equity\"]\n",
    "    credit_metrics_df[\"Asset_Turnover_Ratio\"] = credit_metrics_df[\"Net_Income\"] / credit_metrics_df[\"Total_Assets\"]\n",
    "    credit_metrics_df[\"Inventory_Turnover_Ratio\"] = credit_metrics_df[\"Net_Income\"] / credit_metrics_df[\"Inventory\"]\n",
    "\n",
    "    return credit_metrics_df\n",
    "\n",
    "def calculate_credit_score(credit_metrics_df):\n",
    "    # Define weights for each financial metric\n",
    "    weights = {\n",
    "        \"Debt_Equity_Ratio\": 0.2,\n",
    "        \"Debt_Ratio\": 0.1,\n",
    "        \"Current_Ratio\": 0.1,\n",
    "        \"Quick_Ratio\": 0.1,\n",
    "        \"Interest_Coverage_Ratio\": 0.1,\n",
    "        \"Return_On_Assets\": 0.1,\n",
    "        \"Return_On_Equity\": 0.1,\n",
    "        \"Asset_Turnover_Ratio\": 0.1,\n",
    "        \"Inventory_Turnover_Ratio\": 0.1\n",
    "    }\n",
    "\n",
    "    # Calculate the weighted sum of normalized metrics\n",
    "    credit_score = (credit_metrics_df * pd.Series(weights)).sum(axis=1)\n",
    "\n",
    "    return credit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factors:\n",
    "  @staticmethod\n",
    "  def _create_hedge_portfolio(data: pd.DataFrame, factor_col: str, q=10, direction=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a hedge portfolio, using D10 - D1 returns by default.\n",
    "\n",
    "    Args:\n",
    "      data (pd.DataFrame): Data, must contain columns ['monthid', 'RET', factor_col]\n",
    "      factor_col (str): Name of factor column, i.e. ep1\n",
    "      q (int, optional): Defaults to deciles (10).\n",
    "      direction (int, optional): Either 1 or -1. If 1, we do High - Low. Otherwise, Low - High.\n",
    "\n",
    "    Returns: Return of hedge portfolio at each monthid (i.e. the factor)\n",
    "    \"\"\"\n",
    "    factor = []\n",
    "    index = []  # monthid - 1\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp[factor_col].transform(lambda x: pd.qcut(x.rank(method=\"first\"), 10, labels=False) if not np.isnan(x).all() else x)  # if statement in case all NaN\n",
    "    for monthid, mdata in mth_grp:\n",
    "      # TODO: can add value weighting of returns here if anyone needs it\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == q-1]['RET'].mean()\n",
    "      factor.append(direction * (d10 - d1))\n",
    "      index.append(monthid - 1)\n",
    "    return pd.DataFrame(factor, index=index)\n",
    "  \n",
    "  @staticmethod\n",
    "  def dNoa(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "    \n",
    "    # annual_data = COMP_ANNUAL\n",
    "    fill_zero(annual_data, 'DLC')\n",
    "    fill_zero(annual_data, 'DLTT')\n",
    "    fill_zero(annual_data, 'MIB')\n",
    "    fill_zero(annual_data, 'PSTK')\n",
    "\n",
    "    operating_assets = annual_data[\"AT\"] - annual_data[\"CHE\"]\n",
    "    operating_liabilities = annual_data[\"AT\"] - annual_data[\"DLC\"] - annual_data[\"DLTT\"] - annual_data[\"MIB\"] - annual_data[\"PSTK\"] - annual_data[\"CEQ\"]\n",
    "\n",
    "    Noa = operating_assets - operating_liabilities\n",
    "\n",
    "    lagged_total_assets = annual_data[\"AT\"].shift(1)  # 1-year-lagged total assets\n",
    "    dNoa = (Noa - Noa.shift(1)) / lagged_total_assets\n",
    "\n",
    "    annual_data[\"dNoa\"] = dNoa\n",
    "    annual_data[\"dNoa\"].fillna(0, inplace=True)\n",
    "\n",
    "    data = sqldf(\"SELECT a.*, b.dNoa \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['dNoa'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "\n",
    "  @staticmethod\n",
    "  def Nsi(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "\n",
    "    annual_data[\"CSHO_t_minus_1_adjusted\"] = annual_data[\"CSHO\"] * annual_data[\"AJEX\"]\n",
    "\n",
    "    test_shifted = annual_data.shift(periods=1)\n",
    "    annual_data[\"CSHO_t_minus_2_adjusted\"] = test_shifted[\"CSHO\"] * test_shifted[\"AJEX\"]\n",
    "\n",
    "    annual_data[\"Nsi_u\"] = np.log(annual_data[\"CSHO_t_minus_1_adjusted\"] / annual_data[\"CSHO_t_minus_2_adjusted\"])\n",
    "    annual_data[\"Nsi_u\"].fillna(0, inplace=True)\n",
    "\n",
    "    annual_data[\"Nsi\"] = np.where(annual_data[\"Nsi_u\"] < 0, np.where(annual_data[\"Nsi_u\"] < annual_data[\"Nsi_u\"].quantile(0.5), 1, 2),\n",
    "                             np.where(annual_data[\"Nsi_u\"] == 0, 3,\n",
    "                                      np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.1), 4,\n",
    "                                               np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.2), 5,\n",
    "                                                        np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.3), 6,\n",
    "                                                                 np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.4), 7,\n",
    "                                                                          np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.5), 8,\n",
    "                                                                                   np.where(annual_data[\"Nsi_u\"] <= annual_data[\"Nsi_u\"].quantile(0.6), 9, 10))))))))\n",
    "    annual_data[\"Nsi\"]\n",
    "\n",
    "    data = sqldf(\"SELECT a.*, b.Nsi \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['Nsi'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "\n",
    "  @staticmethod\n",
    "  def dNca(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "    \n",
    "    fill_zero(annual_data, \"IVAO\")\n",
    "\n",
    "    annual_data[\"Nca\"] = annual_data[\"AT\"] - annual_data[\"ACT\"] - annual_data[\"IVAO\"]\n",
    "    annual_data[\"dNca_o\"] = annual_data[\"Nca\"].diff()\n",
    "    annual_data[\"dNca_o\"].fillna(0, inplace=True)\n",
    "\n",
    "    total_assets_t_minus_2 = annual_data['AT'].iloc[-3]\n",
    "\n",
    "    annual_data[\"dNca\"] = annual_data[\"dNca_o\"] / total_assets_t_minus_2\n",
    "\n",
    "    data = sqldf(\"SELECT a.*, b.dNca \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['dNca'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "\n",
    "  @staticmethod\n",
    "  def dFnl(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "    \n",
    "    fill_zero(annual_data, \"DLTT\")\n",
    "    fill_zero(annual_data, \"DLC\")\n",
    "    fill_zero(annual_data, \"PSTK\")\n",
    "\n",
    "    annual_data[\"Fnl\"] = annual_data[\"DLTT\"] + annual_data[\"DLC\"] + annual_data[\"PSTK\"]\n",
    "    annual_data[\"dFnl_o\"] = annual_data[\"Fnl\"].diff()\n",
    "    annual_data[\"dFnl_o\"].fillna(0, inplace=True)\n",
    "    total_assets_t_minus_2 = annual_data['AT'].iloc[-3]\n",
    "\n",
    "    annual_data[\"dFnl\"] = annual_data[\"dFnl_o\"] / total_assets_t_minus_2\n",
    "\n",
    "    data = sqldf(\"SELECT a.*, b.dFnl \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['dFnl'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "\n",
    "  @staticmethod\n",
    "  def creditrisk(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "    \n",
    "    credit_metrics = calculate_credit_metrics(annual_data)\n",
    "    credit_score = calculate_credit_score(credit_metrics)\n",
    "    annual_data[\"cdrk\"] = credit_score\n",
    "\n",
    "    data = sqldf(\"SELECT a.*, b.cdrk \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['cdrk'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def epq1(data):\n",
    "    factor = []\n",
    "    data = data.copy()\n",
    "    data = data[data['IBQ'] >= 0]\n",
    "    data['Epq1'] = data['IBQ'] * 1000000 / data['MKTCAP']\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['Epq1'].transform(lambda x: pd.qcut(x, 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d1 - d10)\n",
    "    return factor\n",
    "  \n",
    "  @staticmethod\n",
    "  def ioca(data):\n",
    "    CPI = pd.read_csv('cpi.csv')\n",
    "    g = .10\n",
    "    d = .15\n",
    "    factor = []\n",
    "    annual_data = sqldf(\"SELECT a.*, b.* \\\n",
    "                            FROM COMP_ANNUAL as a \\\n",
    "                            INNER JOIN INDUSTRY as b \\\n",
    "                            ON a.SIC >= b.lhs and a.SIC <= b.rhs\")\n",
    "    annual_data['year'] = annual_data['FYEAR'].astype(int)\n",
    "    annual_data['LPERMNO'] = annual_data['LPERMNO'].astype(int)\n",
    "    annual_data['XSGA'].fillna(0, inplace=True)\n",
    "    annual_data = pd.merge(annual_data, CPI, on='year')\n",
    "    annual_data.sort_values(by=['LPERMNO', 'year', 'XSGA'], inplace=True)\n",
    "    annual_data.drop_duplicates(subset=['LPERMNO', 'year'], keep='last', inplace=True)\n",
    "    grouped = annual_data.groupby('LPERMNO')\n",
    "    oc = []\n",
    "    for _, grp in grouped:\n",
    "      prev = None\n",
    "      for _, row in grp.iterrows():\n",
    "        if prev == None:\n",
    "          prev = row['XSGA'] / (g+d)\n",
    "        else:\n",
    "          prev = (1 - d) * prev + row['XSGA'] / row['CPI']\n",
    "        oc.append(prev)\n",
    "    annual_data['oc'] = oc\n",
    "    annual_data = annual_data[(annual_data['oc'] > 0) & (annual_data['AT'] > 0)]\n",
    "    annual_data['oca'] = annual_data['oc'] / annual_data['AT']\n",
    "    def yearly_winsorize(srs):\n",
    "      p1, p99 = np.nanpercentile(srs, [1, 99])\n",
    "      return np.clip(srs, p1, p99)\n",
    "    annual_data['ioca'] = annual_data.groupby('year')['oca'].transform(yearly_winsorize)\n",
    "    annual_data['ioca'] = annual_data.groupby('industry')['ioca'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    data = sqldf(\"SELECT a.*, b.ioca \\\n",
    "                  FROM data as a \\\n",
    "                  INNER JOIN annual_data as b \\\n",
    "                  ON a.PERMNO = b.LPERMNO and \\\n",
    "                  ((a.year = b.year and a.month >= 7) or \\\n",
    "                    (a.year = b.year + 1 and a.month <= 6))\")\n",
    "    mth_grp = data.groupby('monthid')\n",
    "    data['rank'] = mth_grp['ioca'].transform(lambda x: pd.qcut(x.rank(method='first'), 10, labels=False))\n",
    "    for _, mdata in mth_grp:\n",
    "      d1 = mdata[mdata['rank'] == 0]['RET'].mean()\n",
    "      d10 = mdata[mdata['rank'] == 9]['RET'].mean()\n",
    "      factor.append(d10 - d1)\n",
    "    return factor\n",
    "  \n",
    "  @staticmethod\n",
    "  def ra26(data):\n",
    "    factor = []\n",
    "    data = data.sort_values(['PERMNO','monthid'])\n",
    "    datagrp = data.groupby('PERMNO')\n",
    "    for _, grp in datagrp:\n",
    "      for mid in grp['monthid']:\n",
    "        tm = grp[grp['monthid'].isin([mid - 24, mid - 36, mid - 48, mid - 60])]\n",
    "        if (len(tm) < 4):\n",
    "          factor.append(np.NaN)\n",
    "        else:\n",
    "          factor.append(tm['RET'].mean())\n",
    "    data['ra26'] = factor\n",
    "    return Factors._create_hedge_portfolio(data, 'ra26')\n",
    "\n",
    "  @staticmethod\n",
    "  def Abr1(data):\n",
    "    factor = []\n",
    "    data = data.copy()\n",
    "\n",
    "    # RET_DAILY = pd.read_sas('ret_daily.sas7bdat', encoding='latin-1')  sas is SLOW\n",
    "    ret_daily = pd.read_parquet(\"ret_daily.parquet\")\n",
    "    vwret = pd.read_sas('us_vwret.sas7bdat', encoding='latin-1').set_index(\"date\")\n",
    "\n",
    "    ret_daily[\"monthid\"] = (ret_daily.DATE.dt.year-1975)*12 + ret_daily.DATE.dt.month\n",
    "\n",
    "    stock_returns = ret_daily.set_index([\"PERMNO\", \"DATE\"]).sort_index()\n",
    "    del ret_daily  # save memory\n",
    "    # faster to pivot first and then do the operations, rather than doing a groupby\n",
    "    stock_returns = stock_returns.pivot_table(values=\"RET\", columns=\"PERMNO\", index=\"DATE\")\n",
    "    stock_returns = stock_returns.sub(vwret.VWRETD, axis=0)  # return over market\n",
    "\n",
    "    # get all 4-period returns from T-3 to T+1\n",
    "    stock_returns = stock_returns.rolling(4).sum().shift(-1)\n",
    "    stock_returns = pd.DataFrame(stock_returns.stack())\n",
    "    stock_returns = stock_returns.rename(columns={0:\"Abr\"})\n",
    "\n",
    "    # Merge on RDQ date so we only have the returns around earnings date\n",
    "    rdqs = data[[\"PERMNO\", \"RDQ\"]].dropna()\n",
    "    rdqs.RDQ = pd.to_datetime(rdqs.RDQ)\n",
    "    rdqs = rdqs.drop_duplicates()\n",
    "    rdqs = rdqs.merge(stock_returns, left_on=[\"RDQ\", \"PERMNO\"], right_index=True, how=\"left\")\n",
    "    del stock_returns  # save memory\n",
    "    # This is the date we use to calculate the monthid to avoid lookahead bias,\n",
    "    # since Abr is calculated using RDQ-3 to RDQ+1 returns.\n",
    "    rdqs[\"RDQ+1\"] = rdqs.RDQ + pd.offsets.BDay(1)\n",
    "    rdqs[\"monthid\"] = (rdqs[\"RDQ+1\"].dt.year-1975)*12 + rdqs[\"RDQ+1\"].dt.month\n",
    "\n",
    "    rdqs = rdqs.set_index([\"PERMNO\", \"monthid\"]).sort_index()\n",
    "\n",
    "    # duplicates can occur where RDQ is in the same month. We drop the first month-PERMNO duplicate\n",
    "    rdqs = rdqs[~rdqs.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    final = pd.DataFrame(index=data.set_index([\"PERMNO\", \"monthid\"]).index).sort_index()\n",
    "    final[\"Abr1\"] = rdqs.Abr\n",
    "    # forward fill only 6 months to avoid stale data\n",
    "    # TODO: could possibly be problems if data skips some monthids, but we still have no lookahead bias, just possible stale data\n",
    "    final = final.groupby(level=0).ffill(limit=6)\n",
    "\n",
    "    data = data.merge(final, how=\"left\", left_on=[\"PERMNO\", \"monthid\"], right_index=True).sort_values(by=['PERMNO', 'monthid'])\n",
    "\n",
    "    return Factors._create_hedge_portfolio(data, \"Abr1\", q=10, direction=1)\n",
    "    # return data\n",
    "\n",
    "  @staticmethod\n",
    "  def e11(data):\n",
    "    \"\"\"\n",
    "    Calculate Îµ11 factor (Residual momentum, prior 11-month returns) with 1 month holding period.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Get a series of rf indexed by date\n",
    "    rf = data.set_index(\"monthid\")[[\"RF\"]].reset_index().drop_duplicates(subset=\"monthid\")\n",
    "    rf = rf.set_index(\"monthid\").sort_index().RF\n",
    "\n",
    "    # Get xret with each permno in a column\n",
    "    xret = data.loc[:,~data.columns.duplicated()].pivot_table(index=\"monthid\", columns=\"PERMNO\", values=\"RET\").sort_index()\n",
    "    xret = xret.sub(rf, axis=0)\n",
    "\n",
    "    # Get ff data with constant\n",
    "    ff_3 = data[[\"monthid\", \"SMB\", \"HML\", \"MKTRF\"]].drop_duplicates(subset=\"monthid\").set_index(\"monthid\").sort_index()\n",
    "    ff_3 = sm.add_constant(ff_3)\n",
    "\n",
    "    from numpy.linalg import pinv\n",
    "\n",
    "    def last_ff_residual(series: pd.Series, ff: pd.DataFrame) -> float:\n",
    "      \"\"\"Computes FF residuals for a series of excess returns.\n",
    "\n",
    "      Args:\n",
    "        series (pd.DataFrame): rolling excess returns. Must have no null values.\n",
    "        ff (pd.DataFrame): factors, must have overlapping index with `series`. All columns are used as factors. Must have constant column added.\n",
    "\n",
    "      Returns: residual on last date T\n",
    "      \"\"\"\n",
    "      # y = series\n",
    "      x = ff.loc[series.index]\n",
    "      \n",
    "      # ffmodel = sm.OLS(y, x).fit()\n",
    "      # residual_values = ffmodel.resid\n",
    "\n",
    "      # sm.OLS is too slow, do it with linear algebra instead\n",
    "      params = pinv(x).dot(series)\n",
    "\n",
    "      # calculate last residual\n",
    "      t_residual = series.iloc[-1] - x.iloc[-1].dot(params)\n",
    "      \n",
    "      return t_residual\n",
    "\n",
    "    residuals = xret.rolling(window=36, min_periods=36).apply(\n",
    "      lambda series: last_ff_residual(series, ff_3)\n",
    "    )\n",
    "    scaled_residuals = residuals / residuals.rolling(36, min_periods=12).std()\n",
    "\n",
    "    # The time T residual momentum is the sum of residual returns for T-12, T-11, ... T-1\n",
    "    # We do this with a rolling 11 period sum, and then shift down by 1\n",
    "    e11 = scaled_residuals.rolling(11).sum().shift()\n",
    "\n",
    "    # Reshape so the columns are [monthid, PERMNO, e11] and merge into data\n",
    "    e11 = pd.DataFrame(e11.stack()).rename(columns={0: \"e11\"})\n",
    "    data = data.merge(e11, how=\"left\", left_on=[\"monthid\", \"PERMNO\"], right_index=True)\n",
    "\n",
    "    return Factors._create_hedge_portfolio(data, \"e11\", q=10, direction=1)\n",
    "\n",
    "  @staticmethod\n",
    "  def Re1(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    ibes_link = pd.read_csv(\"crsp_ibes_link.csv\")  # ibes ticker to PERMNO map  https://wrds-www.wharton.upenn.edu/pages/get-data/linking-suite-wrds/ibes-crsp-link/\n",
    "    ibes_eps = pd.read_csv(\"ibes_eps_estimate.csv\")  # Mean EPS estimates  https://wrds-www.wharton.upenn.edu/pages/get-data/ibes-thomson-reuters/ibes-academic/summary-history/summary-statistics/\n",
    "\n",
    "    ibes_link = ibes_link.dropna()\n",
    "\n",
    "    # Only want USD currency\n",
    "    ibes_eps = ibes_eps[ibes_eps.CURCODE == \"USD\"]\n",
    "\n",
    "    # Get the estimates with the date and permno\n",
    "    eps_est = sqldf(\n",
    "      \"\"\"\n",
    "      select l.permno, e.STATPERS as DATE, e.MEANEST\n",
    "      from ibes_eps e\n",
    "      left join ibes_link l\n",
    "      on e.ticker = l.TICKER and e.STATPERS >= l.sdate and e.STATPERS <= l.edate \n",
    "      \"\"\"\n",
    "    )\n",
    "\n",
    "    # Add monthid\n",
    "    eps_est = eps_est.drop_duplicates(subset=[\"DATE\", \"PERMNO\"])\n",
    "    eps_est.DATE = pd.to_datetime(eps_est.DATE)\n",
    "    eps_est['monthid'] = (eps_est.DATE.dt.year-1975)*12 + eps_est.DATE.dt.month\n",
    "\n",
    "    # Merge close price\n",
    "    eps_est = eps_est.merge(data.set_index([\"monthid\", \"PERMNO\"]).PRC, on=[\"monthid\", \"PERMNO\"])\n",
    "    eps_est = eps_est.set_index([\"PERMNO\", \"monthid\"]).sort_index()\n",
    "\n",
    "    # Calculate factor value for each stock\n",
    "\n",
    "    # TODO: we should make PRC the split adjusted price as our EPS values are split adjusted\n",
    "    re = eps_est.reset_index().set_index(\"monthid\").sort_index().groupby([\"PERMNO\"]).apply(\n",
    "        # 6 period rolling sum of (f_t - f_t-1) / p_t-1, requiring at least 4 consecutive observations\n",
    "        lambda df: ((df.MEANEST - df.MEANEST.shift()) / df.PRC.shift()).rolling(window=6, min_periods=4).sum()\n",
    "    )\n",
    "    eps_est[\"Re1\"] = re\n",
    "    \n",
    "    data = data.merge(eps_est[[\"Re1\"]], how=\"left\", on=[\"monthid\", \"PERMNO\"])\n",
    "    \n",
    "    return Factors._create_hedge_portfolio(data, \"Re1\", q=10, direction=1)\n",
    "  \n",
    "\n",
    "  # D. Investment ==========================================\n",
    "  @staticmethod \n",
    "  def _compute_shifted(data, factor_col, input_col1, input_col2, shift_amt, op='-'):\n",
    "    \"\"\"\n",
    "    for factor calculations that require a shift of data values to avoid\n",
    "    lookahead bias:\n",
    "    Input:\n",
    "      data: DataFrame\n",
    "      factor_col: name of factor to be computed\n",
    "      input_col: name of column used in the factor computation\n",
    "      shift_amt: number of months to lag by for input_col\n",
    "    Return:\n",
    "      a dataframe with cols ['monthid', 'PERMNO', 'factor'] for merge\n",
    "      into the overall data\n",
    "    \"\"\"\n",
    "    permno_grp = data.groupby('PERMNO')\n",
    "    # construct a mini dataframe of shifted values -> (monthid, permno, dRoa1 computed for the relevant month)\n",
    "    factor_values = {'monthid': [], 'PERMNO': [], factor_col:[]}\n",
    "\n",
    "    for _, pdata in permno_grp:\n",
    "      # sort by monthid\n",
    "      pdata = pdata.sort_values('monthid') \n",
    "      # lag Roa to avoid lookahead bias\n",
    "      if (op == '-'):\n",
    "        pdata[factor_col] = pdata[input_col1] - pdata[input_col2].shift(shift_amt)\n",
    "      else:\n",
    "        pdata[factor_col] = pdata[input_col1] / pdata[input_col2].shift(shift_amt)\n",
    "      # add to dataframe\n",
    "      \n",
    "      factor_values['PERMNO'].extend(pdata['PERMNO'].tolist())\n",
    "      factor_values['monthid'].extend(pdata['monthid'].tolist())\n",
    "      factor_values[factor_col].extend(pdata[factor_col].tolist())\n",
    "\n",
    "    factor_values = pd.DataFrame(factor_values)\n",
    "    factor_values['monthid'] = factor_values['monthid'].astype(np.int64)\n",
    "    factor_values['PERMNO'] = factor_values['PERMNO'].astype(np.int64)\n",
    "\n",
    "    return pd.DataFrame(factor_values)\n",
    "    \n",
    "  @staticmethod\n",
    "  def droe1(data):\n",
    "    \"\"\"dRoe: return on equity minus its value from four quarters ago \"\"\"\n",
    "    #   for each month t, sort all stocks into deciles based on their most recent past dRoe\n",
    "    #   monthly decile returns calculated for the current month t\n",
    "\n",
    "    # ROE = NET (RDQ) / Equity (CEQQ)   \n",
    "    data = data.copy()\n",
    "    data['Roe'] = data['NIQ'] / data['CEQQ']\n",
    "\n",
    "    # require that earnings announcement date is after the current fiscal quarter end\n",
    "    data = data[~(data['DATADATE'] > data['RDQ'])]\n",
    "\n",
    "    # compute factor values while accounting for lagged inputs\n",
    "    factor_values = Factors._compute_shifted(data, factor_col='dRoe1', input_col1='Roe', input_col2='Roe', shift_amt=12, op='-')\n",
    "    # merge factor values back into the overall data frame\n",
    "    data = data.merge(factor_values, on=[\"monthid\", \"PERMNO\"])\n",
    "    # then construct decile portfolios\n",
    "    return Factors._create_hedge_portfolio(data, \"dRoe1\", q=10)\n",
    "  \n",
    "  @staticmethod\n",
    "  def droa1(data):\n",
    "    \"\"\" droa1 \"is return on assets minus its value from four quarters ago\" \"\"\"\n",
    "    factor = []\n",
    "    data = data.copy()\n",
    "    data['Roa'] = data['NIQ'] / data['ATQ']\n",
    "\n",
    "    # require that earnings announcement date is after the current fiscal quarter end\n",
    "    data = data[~(data['DATADATE'] > data['RDQ'])]\n",
    "    # compute factor values while accounting for lagged inputs\n",
    "    factor_values = Factors._compute_shifted(data, factor_col='dRoa1', input_col1='Roa', input_col2='Roa', shift_amt=12, op='-')\n",
    "    # merge factor values back into the overall data frame\n",
    "    data = data.merge(factor_values, on=[\"monthid\", \"PERMNO\"])\n",
    "    # then construct decile portfolios\n",
    "    return Factors._create_hedge_portfolio(data, \"dRoa1\", q=10)\n",
    "\n",
    "  @staticmethod\n",
    "  def rnaq1(data):\n",
    "    \"\"\" Rnaq1: Quarterly return on net operating assets \"\"\"\n",
    "\n",
    "    data = data.copy()\n",
    "    # zero out NAs as specified in the paper\n",
    "    fill_na_cols = ['IVAOQ', 'DLCQ', 'DLTTQ', 'MIBQ', 'PSTKQ'] \n",
    "    data[fill_na_cols] = data[fill_na_cols].fillna(0)\n",
    "    # operating assets = ATQ - CHEQ - IVAOQ\n",
    "    data['oa'] = data['ATQ'] - data['CHEQ'] - data['IVAOQ']\n",
    "    # operating liabilities = ATQ - DLCQ - DLTTQ - MIBQ - PSTKQ - CEQQ\n",
    "    data['ol'] = data['ATQ'] - data['DLCQ'] - data['DLTTQ'] - data['MIBQ'] - data['PSTKQ'] - data['CEQQ']\n",
    "    # net operating assets = OA - OL\n",
    "    data['noa'] = data['oa'] - data['ol']\n",
    "\n",
    "    # compute factor values while accounting for lagged inputs - 1 quarter lagged noa\n",
    "    factor_values = Factors._compute_shifted(data, factor_col='rnaq1', input_col1='OIADPQ', input_col2='noa', shift_amt=4, op='/')\n",
    "    # merge factor values back into the overall data frame\n",
    "    data = data.merge(factor_values, on=[\"monthid\", \"PERMNO\"])\n",
    "\n",
    "    return Factors._create_hedge_portfolio(data, 'rnaq1')\n",
    "\n",
    "  @staticmethod\n",
    "  def atoq1(data):\n",
    "    \"\"\" atoq1: quarterly sales divided by 1-quarter-lagged Noa \"\"\"\n",
    "    data = data.copy()\n",
    "    # zero out NAs as specified in the paper\n",
    "    fill_na_cols = ['IVAOQ', 'DLCQ', 'DLTTQ', 'MIBQ', 'PSTKQ'] \n",
    "    data[fill_na_cols] = data[fill_na_cols].fillna(0)\n",
    "    # operating assets = ATQ - CHEQ - IVAOQ\n",
    "    data['oa'] = data['ATQ'] - data['CHEQ'] - data['IVAOQ']\n",
    "    # operating liabilities = ATQ - DLCQ - DLTTQ - MIBQ - PSTKQ - CEQQ\n",
    "    data['ol'] = data['ATQ'] - data['DLCQ'] - data['DLTTQ'] - data['MIBQ'] - data['PSTKQ'] - data['CEQQ']\n",
    "    # net operating assets = OA - OL\n",
    "    data['noa'] = data['oa'] - data['ol']\n",
    "\n",
    "    # compute factor values while accounting for lagged inputs - 1 quarter lagged noa\n",
    "    factor_values = Factors._compute_shifted(data, factor_col='atoq1', input_col1='SALEQ', input_col2='noa', shift_amt=4, op='/')\n",
    "    # merge factor values back into the overall data frame\n",
    "    data = data.merge(factor_values, on=[\"monthid\", \"PERMNO\"])\n",
    "\n",
    "    return Factors._create_hedge_portfolio(data, 'atoq1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assets:\n",
    "  crsp = CRSP.copy()\n",
    "  comp = COMP.copy()\n",
    "  ff4 = FF4.copy()\n",
    "  fact = pd.DataFrame()\n",
    "  data = pd.DataFrame()\n",
    "  train_start = '1975-01-01'\n",
    "  train_end = '2005-12-31'\n",
    "  # test_start = '2006-01-01'\n",
    "  # test_end = '2020-12-31'\n",
    "  factors = {\n",
    "    'epq1': Factors.epq1,\n",
    "    'ioca': Factors.ioca,\n",
    "    'ra26': Factors.ra26,\n",
    "    'e11': Factors.e11,  # comment out for now as this takes ~12min to run\n",
    "    'Re1': Factors.Re1,\n",
    "    'Abr1': Factors.Abr1,\n",
    "\n",
    "    # D. Investment =====================================\n",
    "    'dRoe1': Factors.droe1,\n",
    "    'dRoa1': Factors.droa1,\n",
    "    'rnaq1': Factors.rnaq1,\n",
    "    'atoq1': Factors.atoq1,\n",
    "\n",
    "    'dNoa': Factors.dNoa,\n",
    "    'Nsi': Factors.Nsi,\n",
    "    'dNca': Factors.dNca,\n",
    "    'dFnl' : Factors.dFnl,\n",
    "    'creditrisk': Factors.creditrisk\n",
    "  }\n",
    "  factor_fm = pd.DataFrame(columns=['mean','std_error','tstat'])\n",
    "  \n",
    "  def __init__(self, start_date=None):\n",
    "\n",
    "    # Make testing faster by optiSonally limiting dates\n",
    "    if start_date is not None:\n",
    "      self.crsp = self.crsp[self.crsp.DATE >= start_date]\n",
    "      self.comp = self.comp[self.comp.DATADATE >= start_date]\n",
    "\n",
    "    self.clean_crsp()\n",
    "    self.clean_comp()\n",
    "    self.clean_ff4()\n",
    "    self.illiquidity_filter()\n",
    "    \n",
    "    self.merge_data()\n",
    "    self.gen_factors()\n",
    "    self.fama_macbeth()\n",
    "    \n",
    "  def clean_crsp(self):\n",
    "    self.crsp['PERMNO'] = self.crsp['PERMNO'].astype(int)\n",
    "    self.crsp['year'] = self.crsp['DATE'].dt.year\n",
    "    self.crsp['month'] = self.crsp['DATE'].dt.month\n",
    "    self.crsp['monthid'] = (self.crsp['year']-1975)*12 + self.crsp['month']\n",
    "    self.crsp['PRC'] = self.crsp['PRC'].apply(lambda x: x if x > 0 else x * -1)\n",
    "    \n",
    "  def clean_comp(self):\n",
    "    self.comp['qtrid'] = (self.comp['DATADATE'].dt.year-1975)*12 + self.comp['DATADATE'].dt.month\n",
    "    self.comp['LPERMNO'] = self.comp['LPERMNO'].astype(int)\n",
    "    self.comp.drop(columns=['CONSOL', 'INDFMT', 'DATAFMT', 'POPSRC', 'DATAFQTR', 'DATACQTR', 'CURCDQ', 'COSTAT'], \n",
    "                  inplace=True)\n",
    "    \n",
    "  def clean_ff4(self):\n",
    "    self.ff4['monthid'] = (self.ff4['DATEFF'].dt.year-1975)*12 + self.ff4['DATEFF'].dt.month\n",
    "    self.fact = self.ff4.copy()\n",
    "    \n",
    "  def illiquidity_filter(self):\n",
    "    self.crsp = self.crsp[self.crsp['PRC'] >= 5]\n",
    "    self.crsp['MKTCAP'] = self.crsp['PRC'] * self.crsp['SHROUT'] * 1000\n",
    "    tmp = {}\n",
    "    grp = self.crsp[(self.crsp['month'] == 1) & (self.crsp['MKTCAP'] >= 100000000)].groupby('year')\n",
    "    for yr, group in grp:\n",
    "      tmp[yr] = list(group['PERMNO'])\n",
    "    liquidity = self.crsp.groupby(['year'])['PERMNO'].transform(lambda x: x.isin(tmp[x.name]))\n",
    "    self.crsp = self.crsp[liquidity]\n",
    "    \n",
    "  def merge_data(self):\n",
    "    lhs = self.crsp\n",
    "    rhs = self.comp\n",
    "    self.data = sqldf(\"SELECT a.*, b.* \\\n",
    "                       FROM lhs as a \\\n",
    "                       INNER JOIN rhs as b \\\n",
    "                       ON a.PERMNO = b.LPERMNO and a.monthid >= b.qtrid + 4 and a.monthid <= b.qtrid + 6\")\n",
    "    self.data.drop_duplicates(subset=['PERMNO', 'monthid'], keep='last', inplace=True)\n",
    "    self.data = self.data.loc[:,~self.data.columns.duplicated()].copy()  # remove duplicate col\n",
    "    self.data = pd.merge(self.data, self.ff4, on='monthid')\n",
    "  \n",
    "  def gen_factors(self):\n",
    "    for factor, func in self.factors.items():\n",
    "      try:\n",
    "        self.fact[factor] = func(self.data)\n",
    "      except Exception as e:\n",
    "        print(f\"Error generating {factor=}: {e}\")\n",
    "        print(f\"SKIPPING THIS FACTOR\")\n",
    "    self.fact = pd.merge(self.fact, self.data[['monthid','RET','PERMNO']], on='monthid')\n",
    "  \n",
    "  def fama_macbeth(self):\n",
    "      try:\n",
    "        fmb = self.fact[['DATEFF','monthid','RET',factor,'SMB','HML','MKTRF','UMD']].copy()\n",
    "        fmb = fmb[(fmb['DATEFF'] >= self.train_start) & (fmb['DATEFF'] <= self.train_end)]\n",
    "        fmb.dropna(inplace=True)\n",
    "        result = fama_macbeth(fmb,'monthid','RET',[factor,'SMB','HML','MKTRF','UMD'],intercept=True)\n",
    "        self.factor_fm.loc[factor] = fm_summary(result).loc[factor]\n",
    "      except Exception as e:\n",
    "        print(f\"Error running Fama Macbeth on {factor=}: {e}\")\n",
    "\n",
    "  def gen_factor_betas(self, train_start='1975-01-01', train_end='2005-12-31'):\n",
    "    \"\"\" \n",
    "      testing sample forecasting for linear models\n",
    "    \"\"\"\n",
    "    # initialize a dictionary to store factor betas, where beta columns\n",
    "    # are named {factor}_b\n",
    "    factor_betas = {'PERMNO':[], 'monthid': [], 'alpha':[]}\n",
    "    factor_cols = []\n",
    "    factor_list = list(self.factors.keys())\n",
    "    for factor in self.factors.keys():\n",
    "      factor_b = factor + \"_b\"\n",
    "      # factor_list.append(factor)\n",
    "      factor_cols.append(factor_b)\n",
    "      factor_betas[factor_b] = []\n",
    "\n",
    "    # get coefficients for factors through regression\n",
    "    # combine by stock first, then regress across times\n",
    "    # TODO rolling 36 months\n",
    "    data = self.fact.copy()\n",
    "\n",
    "    # filter data period\n",
    "    data = data[(data['DATEFF'] >= train_start) & (data['DATEFF'] <= train_end)]\n",
    "    permno_grp = data.groupby('PERMNO')\n",
    "\n",
    "    for permno, pdata in permno_grp:\n",
    "      # dropna, TODO consider alternative\n",
    "      pdata = pdata.dropna()\n",
    "      # check number of observations available\n",
    "      if (len(pdata.index) < 12):\n",
    "        continue\n",
    "      # sort by monthid\n",
    "      pdata = pdata.sort_values('monthid') \n",
    "      independents = pdata[factor_list]\n",
    "      # since factors were constructed accounting for lookahead bias (via lags \n",
    "      # when necessary), this step regresses factor values at time t to returns \n",
    "      # at time t\n",
    "      model = linear_model.LinearRegression().fit(independents, pdata[\"RET\"])\n",
    "      # add values to factor coefficients dataframe\n",
    "      factor_betas['PERMNO'].append(pdata['PERMNO'].iloc[0])\n",
    "      factor_betas['monthid'].append(pdata['monthid'].iloc[0])\n",
    "      factor_betas['alpha'].append(model.intercept_)\n",
    "      for i, factor_col in enumerate(factor_cols): \n",
    "        factor_betas[factor_col].append(model.coef_[i])\n",
    "\n",
    "    factor_betas_df = pd.DataFrame(factor_betas)\n",
    "    factor_betas_df.sort_values([\"PERMNO\", \"monthid\"])\n",
    "\n",
    "    return factor_betas_df\n",
    "  \n",
    "\n",
    "  def score_linear(self):\n",
    "    \"\"\" \n",
    "      score stocks during the out of sample period\n",
    "    \"\"\"\n",
    "\n",
    "    # get factor coefficients from regression\n",
    "    data = self.data.copy()\n",
    "    stock_data = self.data.copy()\n",
    "    factor_betas = self.gen_factor_betas()\n",
    "    factor_list = list(self.factors.keys())\n",
    "    data = pd.merge(data, self.fact, on=[\"PERMNO\", \"monthid\"])\n",
    "    data = pd.merge(data, factor_betas, on=[\"PERMNO\", \"monthid\"])\n",
    "\n",
    "    # determine holdings absed on ranks... so for each time point, have a \n",
    "    # list of permnos for top and lowest deciles \n",
    "\n",
    "    # compute returns based on the given formula\n",
    "    data['pre_ret'] = data['alpha']\n",
    "    data['pre_ret'] = data['pre_ret'].fillna(0)\n",
    "    for factor in factor_list:\n",
    "      factor_b = factor+\"_b\"\n",
    "      data['pre_ret'] = data['pre_ret'] + (data[factor] * data[factor_b])\n",
    "    \n",
    "    # rank stocks for each out-of-sample period and thus rebalance at each time\n",
    "    # we want to use the previous month to predict the next, thus use predicted return\n",
    "    # at t-1 for time t's portfolio construction\n",
    "    grouped_data = []\n",
    "    permno_group = data.groupby('PERMNO')\n",
    "    for _, pdata in permno_group:\n",
    "      # sort by monthid, then shift to get t-1 return\n",
    "      pdata = pdata.sort_values([\"monthid\"])\n",
    "      pdata['pre_ret_t1'] = pdata['pre_ret'] # TODO issue here\n",
    "      # pdata['pre_ret_t1'] = pdata['pre_ret'].shift(1)\n",
    "      grouped_data.append(pdata)\n",
    "    # merge back into overall data \n",
    "    data = pd.concat(grouped_data)\n",
    "\n",
    "    # store what permnos to construct the long / short portfolio with \n",
    "    # initialize a dataframe to track decile 10 and 1 portfolios\n",
    "    # rank stocks to deciles based on the values calculated from the predication equation\n",
    "    data['rank'] = data['pre_ret_t1'].transform(lambda x: pd.qcut(x.rank(method=\"first\"), 10, labels=False) if not np.isnan(x).all() else x)  # if statement in case all NaN\n",
    "    decile_permnos = {'monthid': [], 'd1_permnos':[],  'd10_permnos': []}\n",
    "\n",
    "    mth_group = data.groupby('monthid')\n",
    "    for month, mdata in mth_group:\n",
    "      # skipping ones with very little data\n",
    "      if (len(mdata['pre_ret_t1']) < 12):\n",
    "        continue\n",
    "      # construct equal weighted decile portfolios\n",
    "      decile_permnos['monthid'].append(mdata['monthid'].iloc[0])\n",
    "      decile_permnos['d1_permnos'].append(mdata[mdata['rank']==0]['PERMNO'])\n",
    "      decile_permnos['d10_permnos'].append(mdata[mdata['rank']==9]['PERMNO'])\n",
    "\n",
    "    decile_permnos_df = pd.DataFrame(decile_permnos)\n",
    "    decile_permnos_df.sort_values(['monthid'])\n",
    "\n",
    "    # for each time point, we have the permnos of the 1st and 10th decile \n",
    "    # construct equal weighted portfolios \n",
    "    decile_rets = {'monthid': [], 'd1_ret': [], 'd10_ret': []}\n",
    "    mth_groups = stock_data.groupby('monthid')\n",
    "    for _, mdata in mth_groups:\n",
    "      monthid = mdata['monthid'].iloc[0]\n",
    "      # skipping months without data\n",
    "      if not (decile_permnos_df['monthid'].isin([monthid]).any()):\n",
    "        continue\n",
    "      # locate the current monthid's permnos, then filter to those permno and id, and take the mean of the return column \n",
    "      d1_permnos = decile_permnos_df.loc[decile_permnos_df['monthid'] == monthid, 'd1_permnos'].values[0]\n",
    "      d10_permnos = decile_permnos_df.loc[decile_permnos_df['monthid'] == monthid, 'd10_permnos'].values[0]\n",
    "      # filter returns for decile 1 and decile 10 permnos\n",
    "      d1_rets = mdata[mdata['PERMNO'].isin(d1_permnos)]['RET']\n",
    "      d10_rets = mdata[mdata['PERMNO'].isin(d10_permnos)]['RET']\n",
    "      # equal weight d1 and d10 portfolios\n",
    "      decile_rets['monthid'].append(monthid)\n",
    "      decile_rets['d1_ret'].append(d1_rets.mean())\n",
    "      decile_rets['d10_ret'].append(d10_rets.mean())\n",
    "    # return a dataframe of decile returns\n",
    "    decile_rets_df = pd.DataFrame(decile_rets)\n",
    "    decile_rets_df.sort_values([\"monthid\"], inplace=True)\n",
    "    decile_rets_df['d10-d1_ret'] = decile_rets_df['d10_ret'] - decile_rets_df['d1_ret']\n",
    "    return decile_rets_df\n",
    " \n",
    "  def performance_analytics(self):\n",
    "    decile_rets = self.score_linear()\n",
    "\n",
    "    # raw return\n",
    "\n",
    "    # sharpe ratio\n",
    "\n",
    "    # CAPM alpha \n",
    "\n",
    "    # 4-Factor alpha \n",
    "\n",
    "    # Information ratio \n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hughj\\AppData\\Local\\Temp\\ipykernel_14944\\3478052425.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.crsp['MKTCAP'] = self.crsp['PRC'] * self.crsp['SHROUT'] * 1000\n",
      "C:\\Users\\hughj\\AppData\\Local\\Temp\\ipykernel_14944\\395935338.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annual_data['oca'] = annual_data['oc'] / annual_data['AT']\n",
      "C:\\Users\\hughj\\AppData\\Local\\Temp\\ipykernel_14944\\395935338.py:279: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annual_data['ioca'] = annual_data.groupby('year')['oca'].transform(yearly_winsorize)\n",
      "C:\\Users\\hughj\\AppData\\Local\\Temp\\ipykernel_14944\\395935338.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  annual_data['ioca'] = annual_data.groupby('industry')['ioca'].transform(lambda x: (x - x.mean()) / x.std())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAN values for DLC: 6332\n",
      "NAN values for DLC filled: 0 remaining\n",
      "NAN values for DLTT: 5291\n",
      "NAN values for DLTT filled: 0 remaining\n",
      "NAN values for MIB: 18809\n",
      "NAN values for MIB filled: 0 remaining\n",
      "NAN values for PSTK: 9007\n",
      "NAN values for PSTK filled: 0 remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hughj\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAN values for IVAO: 21261\n",
      "NAN values for IVAO filled: 0 remaining\n",
      "NAN values for DLTT: 5291\n",
      "NAN values for DLTT filled: 0 remaining\n",
      "NAN values for DLC: 6332\n",
      "NAN values for DLC filled: 0 remaining\n",
      "NAN values for PSTK: 9007\n",
      "NAN values for PSTK filled: 0 remaining\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14944\\3478052425.py\u001b[0m in \u001b[0;36mfama_macbeth\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mfmb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfact\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATEFF'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'monthid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RET'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SMB'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'HML'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MKTRF'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'UMD'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mfmb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATEFF'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DATEFF'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'factor' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14944\\4086888647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAssets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14944\\3478052425.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, start_date)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_factors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfama_macbeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclean_crsp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14944\\3478052425.py\u001b[0m in \u001b[0;36mfama_macbeth\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor_fm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfm_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error running Fama Macbeth on {factor=}: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgen_factor_betas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1975-01-01'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2005-12-31'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'factor' is not defined"
     ]
    }
   ],
   "source": [
    "assets = Assets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14944\\1073105759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0massets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'assets' is not defined"
     ]
    }
   ],
   "source": [
    "assets.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATEFF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>MKTRF</th>\n",
       "      <th>RF</th>\n",
       "      <th>UMD</th>\n",
       "      <th>monthid</th>\n",
       "      <th>epq1</th>\n",
       "      <th>RET</th>\n",
       "      <th>PERMNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1975-01-31</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.1382</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>0.231061</td>\n",
       "      <td>10006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975-01-31</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.1382</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>0.337349</td>\n",
       "      <td>10102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1975-01-31</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.1382</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>10137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1975-01-31</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.1382</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>0.136564</td>\n",
       "      <td>10145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1975-01-31</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>-0.1382</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>0.215447</td>\n",
       "      <td>10161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242503</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>552</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.143199</td>\n",
       "      <td>93397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242504</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>552</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.109665</td>\n",
       "      <td>93423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242505</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>552</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.076239</td>\n",
       "      <td>93426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242506</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>552</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.135851</td>\n",
       "      <td>93427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242507</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>552</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.243252</td>\n",
       "      <td>93436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242508 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DATEFF     SMB     HML   MKTRF      RF     UMD  monthid      epq1  \\\n",
       "0       1975-01-31  0.1114  0.0828  0.1366  0.0058 -0.1382        1 -0.026479   \n",
       "1       1975-01-31  0.1114  0.0828  0.1366  0.0058 -0.1382        1 -0.026479   \n",
       "2       1975-01-31  0.1114  0.0828  0.1366  0.0058 -0.1382        1 -0.026479   \n",
       "3       1975-01-31  0.1114  0.0828  0.1366  0.0058 -0.1382        1 -0.026479   \n",
       "4       1975-01-31  0.1114  0.0828  0.1366  0.0058 -0.1382        1 -0.026479   \n",
       "...            ...     ...     ...     ...     ...     ...      ...       ...   \n",
       "1242503 2020-12-31  0.0489 -0.0151  0.0463  0.0001 -0.0232      552  0.032500   \n",
       "1242504 2020-12-31  0.0489 -0.0151  0.0463  0.0001 -0.0232      552  0.032500   \n",
       "1242505 2020-12-31  0.0489 -0.0151  0.0463  0.0001 -0.0232      552  0.032500   \n",
       "1242506 2020-12-31  0.0489 -0.0151  0.0463  0.0001 -0.0232      552  0.032500   \n",
       "1242507 2020-12-31  0.0489 -0.0151  0.0463  0.0001 -0.0232      552  0.032500   \n",
       "\n",
       "              RET  PERMNO  \n",
       "0        0.231061   10006  \n",
       "1        0.337349   10102  \n",
       "2        0.333333   10137  \n",
       "3        0.136564   10145  \n",
       "4        0.215447   10161  \n",
       "...           ...     ...  \n",
       "1242503  0.143199   93397  \n",
       "1242504  0.109665   93423  \n",
       "1242505  0.076239   93426  \n",
       "1242506  0.135851   93427  \n",
       "1242507  0.243252   93436  \n",
       "\n",
       "[1242508 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets.fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std_error</th>\n",
       "      <th>tstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epq1</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>6.958548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean  std_error     tstat\n",
       "epq1  0.001333   0.000192  6.958548"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets.factor_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assets.fact.to_csv('assets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>monthid</th>\n",
       "      <th>alpha</th>\n",
       "      <th>dRoe1_b</th>\n",
       "      <th>dRoa1_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>277</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>1.712894</td>\n",
       "      <td>-1.570403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10006</td>\n",
       "      <td>13</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.382421</td>\n",
       "      <td>0.128447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10010</td>\n",
       "      <td>205</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>1.516376</td>\n",
       "      <td>1.267212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10016</td>\n",
       "      <td>139</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>1.444490</td>\n",
       "      <td>-1.667642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10017</td>\n",
       "      <td>140</td>\n",
       "      <td>0.021820</td>\n",
       "      <td>3.328701</td>\n",
       "      <td>-2.908152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7880</th>\n",
       "      <td>93060</td>\n",
       "      <td>229</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>-1.723009</td>\n",
       "      <td>3.150263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7881</th>\n",
       "      <td>93105</td>\n",
       "      <td>265</td>\n",
       "      <td>0.025785</td>\n",
       "      <td>1.284045</td>\n",
       "      <td>-0.693150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7882</th>\n",
       "      <td>93156</td>\n",
       "      <td>181</td>\n",
       "      <td>0.060539</td>\n",
       "      <td>-6.358989</td>\n",
       "      <td>2.396513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>93201</td>\n",
       "      <td>133</td>\n",
       "      <td>-0.011738</td>\n",
       "      <td>2.929501</td>\n",
       "      <td>-0.452179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>93220</td>\n",
       "      <td>169</td>\n",
       "      <td>-0.035598</td>\n",
       "      <td>0.048146</td>\n",
       "      <td>1.826888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7885 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PERMNO  monthid     alpha   dRoe1_b   dRoa1_b\n",
       "0      10002      277  0.007957  1.712894 -1.570403\n",
       "1      10006       13  0.013049  0.382421  0.128447\n",
       "2      10010      205  0.005303  1.516376  1.267212\n",
       "3      10016      139  0.004749  1.444490 -1.667642\n",
       "4      10017      140  0.021820  3.328701 -2.908152\n",
       "...      ...      ...       ...       ...       ...\n",
       "7880   93060      229  0.021300 -1.723009  3.150263\n",
       "7881   93105      265  0.025785  1.284045 -0.693150\n",
       "7882   93156      181  0.060539 -6.358989  2.396513\n",
       "7883   93201      133 -0.011738  2.929501 -0.452179\n",
       "7884   93220      169 -0.035598  0.048146  1.826888\n",
       "\n",
       "[7885 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets.gen_factor_betas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>monthid</th>\n",
       "      <th>d1_ret</th>\n",
       "      <th>d10_ret</th>\n",
       "      <th>d10-d1_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.102334</td>\n",
       "      <td>0.270744</td>\n",
       "      <td>0.168409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0.018328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>-0.146495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>-0.051641</td>\n",
       "      <td>0.302512</td>\n",
       "      <td>0.354153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>-0.066780</td>\n",
       "      <td>0.296429</td>\n",
       "      <td>0.363209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>349</td>\n",
       "      <td>-0.027551</td>\n",
       "      <td>0.227515</td>\n",
       "      <td>0.255066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>352</td>\n",
       "      <td>-0.085451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>355</td>\n",
       "      <td>-0.124547</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.143647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>361</td>\n",
       "      <td>-0.104324</td>\n",
       "      <td>0.069021</td>\n",
       "      <td>0.173345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    monthid    d1_ret   d10_ret  d10-d1_ret\n",
       "0        13  0.102334  0.270744    0.168409\n",
       "1        25  0.018328       NaN         NaN\n",
       "2        37 -0.146495       NaN         NaN\n",
       "3        49 -0.051641  0.302512    0.354153\n",
       "4        61 -0.066780  0.296429    0.363209\n",
       "..      ...       ...       ...         ...\n",
       "56      340       NaN       NaN         NaN\n",
       "57      349 -0.027551  0.227515    0.255066\n",
       "58      352 -0.085451  0.000000    0.085451\n",
       "59      355 -0.124547  0.019101    0.143647\n",
       "60      361 -0.104324  0.069021    0.173345\n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets.score_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
